{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4224d9-3f68-41b7-93e4-160a2a766c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q modelscope open_clip_torch pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0415bd-7422-4407-b937-0170585ea2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m  ERROR: HTTP error 404 while getting http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino-2023.1.0.dev20230801-11888-cp310-cp310-manylinux_2_31_x86_64.whl\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not install requirement openvino==2023.1.0.dev20230801 from http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino-2023.1.0.dev20230801-11888-cp310-cp310-manylinux_2_31_x86_64.whl because of HTTP error 404 Client Error: Not Found for url: http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino-2023.1.0.dev20230801-11888-cp310-cp310-manylinux_2_31_x86_64.whl for URL http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino-2023.1.0.dev20230801-11888-cp310-cp310-manylinux_2_31_x86_64.whl\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino-2023.1.0.dev20230801-11888-cp310-cp310-manylinux_2_31_x86_64.whl http://10.211.120.125/openvino_ci/private_builds/dldt/master/pre_commit/25707c65c8f3b3de2915e6f4aa0ea485593adf5d/private_linux_ubuntu_20_04_release/wheels/openvino_dev-2023.1.0.dev20230801-11888-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee1ba90-50d5-421a-b4fc-ad37e9fe1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:05:22,363 - modelscope - INFO - PyTorch version 1.13.1+cpu Found.\n",
      "2023-08-07 21:05:22,370 - modelscope - INFO - TensorFlow version 2.12.0 Found.\n",
      "2023-08-07 21:05:22,371 - modelscope - INFO - Loading ast index from /home/itrushkin/.cache/modelscope/ast_indexer\n",
      "2023-08-07 21:05:22,618 - modelscope - INFO - Loading done! Current index file version is 1.7.1, with md5 e847f1b015df372fdb9573c01b4a727f and a total number of 861 components indexed\n",
      "2023-08-07 21:05:33,898 - modelscope - INFO - Model revision not specified, use the latest revision: v1.1.0\n",
      "2023-08-07 21:05:35,637 - modelscope - INFO - initiate model from /home/itrushkin/.cache/modelscope/hub/damo/text-to-video-synthesis\n",
      "2023-08-07 21:05:35,638 - modelscope - INFO - initiate model from location /home/itrushkin/.cache/modelscope/hub/damo/text-to-video-synthesis.\n",
      "2023-08-07 21:05:35,644 - modelscope - INFO - initialize model from /home/itrushkin/.cache/modelscope/hub/damo/text-to-video-synthesis\n",
      "2023-08-07 21:07:24,449 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "WARNING:modelscope:No preprocessor field found in cfg.\n",
      "2023-08-07 21:07:24,451 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "WARNING:modelscope:No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-08-07 21:07:24,451 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/home/itrushkin/.cache/modelscope/hub/damo/text-to-video-synthesis'}. trying to build by task and model information.\n",
      "WARNING:modelscope:Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/home/itrushkin/.cache/modelscope/hub/damo/text-to-video-synthesis'}. trying to build by task and model information.\n",
      "2023-08-07 21:07:24,452 - modelscope - WARNING - No preprocessor key ('latent-text-to-video-synthesis', 'text-to-video-synthesis') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
      "WARNING:modelscope:No preprocessor key ('latent-text-to-video-synthesis', 'text-to-video-synthesis') found in PREPROCESSOR_MAP, skip building preprocessor.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "from pathlib import Path\n",
    "\n",
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.models.multi_modal.video_synthesis.text_to_video_synthesis_model import FrozenOpenCLIPEmbedder\n",
    "from modelscope.outputs import OutputKeys\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "\n",
    "torch.load = functools.partial(torch.load, map_location='cpu')\n",
    "p = pipeline('text-to-video-synthesis', 'damo/text-to-video-synthesis', device='cpu')\n",
    "p.model.clip_encoder = FrozenOpenCLIPEmbedder(version=os.path.join(p.model.model_dir, p.model.config.model.model_args.ckpt_clip), layer='penultimate', device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd85b43-3784-485b-b5f4-40ba5a628939",
   "metadata": {},
   "source": [
    "## Conversion via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66a37f12-344e-47fa-a381-baa22a9c80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple, Optional\n",
    "\n",
    "\n",
    "class ModelInfo(NamedTuple):\n",
    "    model: torch.nn.Module\n",
    "    example_input: torch.Tensor | Tuple[torch.Tensor]\n",
    "    xml_path: str\n",
    "\n",
    "    def convert_and_serialize(self):\n",
    "        if not Path(self.xml_path).exists():\n",
    "            ov_model = mo.convert_model(self.model, example_input=self.example_input)\n",
    "            serialize(ov_model, self.xml_path)\n",
    "        else:\n",
    "            print(f'{Path(self.xml_path).resolve()} exists.')\n",
    "\n",
    "    def compile(self, device: Optional[str] = None):\n",
    "        if not Path(self.xml_path).exists():\n",
    "            print(\"Model is not converted yet. Converting...\")\n",
    "            self.convert_and_serialize()\n",
    "        ov_model = core.read_model(self.xml_path)\n",
    "        return core.compile_model(ov_model)\n",
    "        \n",
    "\n",
    "\n",
    "models = {\n",
    "    \"unet\": ModelInfo(\n",
    "        p.model.sd_model,\n",
    "        {\"x\": torch.randn(1, 4, 16, 32, 32), \"t\": torch.randn(1), \"y\": torch.randn(1, 77, 1024)},\n",
    "        \"unet/unet.xml\",\n",
    "    ),\n",
    "    \"decoder\": ModelInfo(\n",
    "        p.model.autoencoder.decoder, torch.randn(16, 4, 32, 32), \"autoencoder/decoder.xml\"\n",
    "    ),\n",
    "    \"post_quant_conv\": ModelInfo(\n",
    "        p.model.autoencoder.post_quant_conv,\n",
    "        torch.randn(16, 4, 32, 32),\n",
    "        \"autoencoder/post_quant_conv.xml\",\n",
    "    ),\n",
    "    \"token_embedding\": ModelInfo(\n",
    "        p.model.clip_encoder.model.token_embedding,\n",
    "        torch.randn(1, 77),\n",
    "        \"clip_encoder/token_embedding.xml\",\n",
    "    ),\n",
    "    \"ln_final\": ModelInfo(\n",
    "        p.model.clip_encoder.model.ln_final,\n",
    "        torch.randn(1, 77, 1024),\n",
    "        \"clip_encoder/ln_final.xml\"\n",
    "    )\n",
    "}\n",
    "for i, r in enumerate(p.model.clip_encoder.model.transformer.resblocks):\n",
    "    models[f\"resblock_{i}\"] = ModelInfo(\n",
    "        r,\n",
    "        {\"q_x\": torch.randn(77, 1, 1024), \"attn_mask\": torch.randn(77, 77)},\n",
    "        f\"clip_encoder/resblock_{i}.xml\",\n",
    "    )\n",
    "RESBLOCKS_COUNT = len(p.model.clip_encoder.model.transformer.resblocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a04fd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(ov_model: CompiledModel, input: torch.Tensor):\n",
    "    out = ov_model(input)\n",
    "    out = out[ov_model.outputs[0]]\n",
    "    out = torch.tensor(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37e55dd-e090-40bd-a5ba-487b993f78f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/unet/unet.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/autoencoder/decoder.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/autoencoder/post_quant_conv.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/token_embedding.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/ln_final.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_0.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_1.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_2.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_3.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_4.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_5.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_6.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_7.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_8.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_9.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_10.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_11.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_12.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_13.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_14.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_15.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_16.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_17.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_18.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_19.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_20.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_21.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_22.xml exists.\n",
      "/home/itrushkin/repos/openvino_notebooks/notebooks/244-modelscope-damo-text2video/clip_encoder/resblock_23.xml exists.\n"
     ]
    }
   ],
   "source": [
    "for name, model_info in models.items():\n",
    "    model_info.convert_and_serialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca5c29c-5a41-44cd-a194-3ab138d43612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "def p_mean_variance(self,\n",
    "                    xt,\n",
    "                    t,\n",
    "                    model,\n",
    "                    model_kwargs={},\n",
    "                    clamp=None,\n",
    "                    percentile=None,\n",
    "                    guide_scale=None):\n",
    "    r\"\"\"Distribution of p(x_{t-1} | x_t).\n",
    "    \"\"\"\n",
    "    def _i(tensor, t, x):\n",
    "        r\"\"\"Index tensor using t and format the output according to x.\n",
    "        \"\"\"\n",
    "        tensor = tensor.to(x.device)\n",
    "        shape = (x.size(0), ) + (1, ) * (x.ndim - 1)\n",
    "        return tensor[t].view(shape).to(x)\n",
    "    \n",
    "    # predict distribution\n",
    "    if guide_scale is None:\n",
    "        \n",
    "        out = model({\"x\": xt, \"t\": self._scale_timesteps(t), **model_kwargs})\n",
    "    else:\n",
    "        # classifier-free guidance\n",
    "        # (model_kwargs[0]: conditional kwargs; model_kwargs[1]: non-conditional kwargs)\n",
    "        assert isinstance(model_kwargs, list) and len(model_kwargs) == 2\n",
    "        y_out = call(model, {\"x\": xt, \"t\": self._scale_timesteps(t), **model_kwargs[0]})\n",
    "        u_out = call(model, {\"x\": xt, \"t\": self._scale_timesteps(t), **model_kwargs[1]})\n",
    "        dim = y_out.size(1) if self.var_type.startswith(\n",
    "                'fixed') else y_out.size(1) // 2\n",
    "        a = u_out[:, :dim]\n",
    "        b = guide_scale * (y_out[:, :dim] - u_out[:, :dim])\n",
    "        c = y_out[:, dim:]\n",
    "        out = torch.cat([a + b, c], dim=1)\n",
    "\n",
    "    # compute variance\n",
    "    if self.var_type == 'fixed_small':\n",
    "        var = _i(self.posterior_variance, t, xt)\n",
    "        log_var = _i(self.posterior_log_variance_clipped, t, xt)\n",
    "\n",
    "    # compute mean and x0\n",
    "    if self.mean_type == 'eps':\n",
    "        x0 = _i(self.sqrt_recip_alphas_cumprod, t, xt) * xt - _i(\n",
    "            self.sqrt_recipm1_alphas_cumprod, t, xt) * out\n",
    "        mu, _, _ = self.q_posterior_mean_variance(x0, xt, t)\n",
    "\n",
    "    # restrict the range of x0\n",
    "    if percentile is not None:\n",
    "        assert percentile > 0 and percentile <= 1  # e.g., 0.995\n",
    "        s = torch.quantile(\n",
    "            x0.flatten(1).abs(), percentile,\n",
    "            dim=1).clamp_(1.0).view(-1, 1, 1, 1)\n",
    "        x0 = torch.min(s, torch.max(-s, x0)) / s\n",
    "    elif clamp is not None:\n",
    "        x0 = x0.clamp(-clamp, clamp)\n",
    "    return mu, var, log_var, x0\n",
    "\n",
    "p.model.diffusion.p_mean_variance = partial(p_mean_variance, p.model.diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e764c76a-876b-484d-b221-2b44f2f5a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def forward(self, input):\n",
    "    r\"\"\"\n",
    "    The entry function of text to image synthesis task.\n",
    "    1. Using diffusion model to generate the video's latent representation.\n",
    "    2. Using vqgan model (autoencoder) to decode the video's latent representation to visual space.\n",
    "\n",
    "    Args:\n",
    "        input (`Dict[Str, Any]`):\n",
    "            The input of the task\n",
    "    Returns:\n",
    "        A generated video (as pytorch tensor).\n",
    "    \"\"\"\n",
    "    y = input['text_emb']\n",
    "    zero_y = input['text_emb_zero']\n",
    "    context = torch.cat([zero_y, y], dim=0).to(self.device)\n",
    "    # synthesis\n",
    "    num_sample = 1  # here let b = 1\n",
    "    max_frames = self.config.model.model_args.max_frames\n",
    "    latent_h, latent_w = input['out_height'] // 8, input[\n",
    "        'out_width'] // 8\n",
    "    x0 = self.diffusion.ddim_sample_loop(\n",
    "        noise=torch.randn(num_sample, 4, max_frames, latent_h,\n",
    "                          latent_w).to(\n",
    "                              self.device),  # shape: b c f h w\n",
    "        model=self.sd_model,\n",
    "        model_kwargs=[{\n",
    "            'y':\n",
    "            context[1].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "        }, {\n",
    "            'y':\n",
    "            context[0].unsqueeze(0).repeat(num_sample, 1, 1)\n",
    "        }],\n",
    "        guide_scale=9.0,\n",
    "        ddim_timesteps=50,\n",
    "        eta=0.0)\n",
    "\n",
    "    scale_factor = 0.18215\n",
    "    video_data = 1. / scale_factor * x0\n",
    "    bs_vd = video_data.shape[0]\n",
    "    video_data = rearrange(video_data, 'b c f h w -> (b f) c h w')\n",
    "    video_data = self.autoencoder.decode(video_data)\n",
    "    video_data = rearrange(\n",
    "        video_data, '(b f) c h w -> b c f h w', b=bs_vd)\n",
    "    return video_data.type(torch.float32)\n",
    "p.model.forward = partial(forward, p.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a417b88c-2b88-41da-b1fd-55bde14d8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVAutoencoderKL:\n",
    "    def __init__(self):\n",
    "        self.decoder = models['decoder'].compile()\n",
    "        self.post_quant_conv = models['post_quant_conv'].compile()\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = call(self.post_quant_conv, z)\n",
    "        dec = call(self.decoder, z)\n",
    "        return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25ba11c7-e140-41f5-a19c-e5115c73895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "class OVFrozenOpenClipEncoder:\n",
    "    def __init__(self, positional_embedding: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        self.token_embedding = models['token_embedding'].compile()\n",
    "        self.transformer_resblocks = [models[f'resblock_{i}'].compile() for i in range(RESBLOCKS_COUNT)]\n",
    "        self.layer_idx = 1\n",
    "        self.positional_embedding = positional_embedding\n",
    "        self.attn_mask = attn_mask\n",
    "        self.ln_final = models['ln_final'].compile()\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = open_clip.tokenize(text)\n",
    "        z = self.encode_with_transformer(tokens)\n",
    "        return z\n",
    "\n",
    "    def encode_with_transformer(self, text):\n",
    "        x = call(self.token_embedding, text)  # [batch_size, n_ctx, d_model]\n",
    "        x = x + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.text_transformer_forward(x, attn_mask=self.attn_mask)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = call(self.ln_final, x)\n",
    "        return x\n",
    "\n",
    "    def text_transformer_forward(self, x: torch.Tensor, attn_mask=None):\n",
    "        for i, r in enumerate(self.transformer_resblocks):\n",
    "            if i == len(self.transformer_resblocks) - self.layer_idx:\n",
    "                break\n",
    "            x = call(r, {\"q_x\": x, \"attn_mask\": attn_mask})\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ca305c-9f23-415f-9d96-5c0ec229123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import tempfile\n",
    "from modelscope.outputs import OutputKeys\n",
    "import cv2\n",
    "\n",
    "def preprocess(self, input, **preprocess_params):\n",
    "    text_emb = self.model.clip_encoder(input['text'])\n",
    "    text_emb_zero = self.model.clip_encoder('')\n",
    "    out_height = input['height'] if 'height' in input else 256\n",
    "    out_width = input['width'] if 'height' in input else 256\n",
    "    return {\n",
    "        'text_emb': text_emb,\n",
    "        'text_emb_zero': text_emb_zero,\n",
    "        'out_height': out_height,\n",
    "        'out_width': out_width\n",
    "    }\n",
    "\n",
    "# def postprocess(self, inputs, **post_params):\n",
    "#     video = tensor2vid(inputs['video'])\n",
    "#     output_video_path = post_params.get('output_video', None)\n",
    "#     temp_video_file = False\n",
    "#     if output_video_path is None:\n",
    "#         output_video_path = tempfile.NamedTemporaryFile(suffix='.mp4').name\n",
    "#         temp_video_file = True\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "#     h, w, c = video[0].shape\n",
    "#     video_writer = cv2.VideoWriter(\n",
    "#         output_video_path, fourcc, fps=8, frameSize=(w, h))\n",
    "#     for i in range(len(video)):\n",
    "#         img = cv2.cvtColor(video[i], cv2.COLOR_RGB2BGR)\n",
    "#         video_writer.write(img)\n",
    "#     video_writer.release()\n",
    "#     if temp_video_file:\n",
    "#         video_file_content = b''\n",
    "#         with open(output_video_path, 'rb') as f:\n",
    "#             video_file_content = f.read()\n",
    "#         os.remove(output_video_path)\n",
    "#         return {OutputKeys.OUTPUT_VIDEO: video_file_content}\n",
    "#     else:\n",
    "#         return {OutputKeys.OUTPUT_VIDEO: output_video_path}\n",
    "\n",
    "p.preprocess = partial(preprocess, p)\n",
    "# p.postprocess = partial(postprocess, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21f4cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_embedding = p.model.clip_encoder.model.positional_embedding\n",
    "attn_mask = p.model.clip_encoder.model.attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512b9a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del p.model.sd_model\n",
    "del p.model.autoencoder\n",
    "del p.model.clip_encoder\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8abff21a-a4e2-436b-9533-36932536a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "core=Core()\n",
    "\n",
    "p.model.sd_model = models['unet'].compile()\n",
    "p.model.autoencoder = OVAutoencoderKL()\n",
    "p.model.clip_encoder = OVFrozenOpenClipEncoder(positional_embedding, attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85e95af2-158c-4216-9914-7f77d8dd2427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_video': 'output.mp4'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p({'text': 'Little panda eating bamboo'}, output_video='output.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c90ed-8d9b-4adb-b9ba-0fb0d3b4c07f",
   "metadata": {},
   "source": [
    "## Conversion via ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbeb405-31e7-4a43-b6aa-9684bf4c35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET_PATH = Path('unet')\n",
    "# UNET_PATH.mkdir(exist_ok=True)\n",
    "# torch.onnx.export(p.model.sd_model, (torch.randn(1, 4, 16, 32, 32),torch.randn(1), torch.randn(1, 77, 1024)), str(UNET_PATH / 'unet.onnx'))\n",
    "# unet = mo.convert_model(str(UNET_PATH / 'unet.onnx'))\n",
    "# serialize(unet, str(UNET_PATH / 'unet.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fefb8f5-1f6f-4690-a57e-8784ec79baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTOENCODER_PATH = Path('autoencoder')\n",
    "# Path.mkdir(AUTOENCODER_PATH, exist_ok=True)\n",
    "# torch.onnx.export(p.model.autoencoder.decoder, torch.randn(16, 4, 32, 32), str(AUTOENCODER_PATH / 'decoder.onnx'))\n",
    "# torch.onnx.export(p.model.autoencoder.post_quant_conv, torch.randn(16, 4, 32, 32), str(AUTOENCODER_PATH / 'post_quant_conv.onnx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c77097-672e-417d-b1d5-27cbbda9a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP_ENCODER_PATH = Path('clip_encoder')\n",
    "# CLIP_ENCODER_PATH.mkdir(exist_ok=True)\n",
    "# torch.onnx.export(p.model.clip_encoder, 'sample text', str(CLIP_ENCODER_PATH / 'clip_encoder.onnx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17df481-8198-42c7-aed9-c7468b30bb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ov_clip_encoder = mo.convert_model(str(CLIP_ENCODER_PATH / 'clip_encoder.onnx'))\n",
    "# serialize(ov_clip_encoder, str(CLIP_ENCODER_PATH / 'clip_encoder.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944af3a-5946-4d30-91a2-f8c73ff8df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ov_decoder = mo.convert_model(str(AUTOENCODER_PATH / 'decoder.onnx'))\n",
    "# serialize(ov_decoder, str(AUTOENCODER_PATH / 'decoder.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d03826-0ee1-4aea-a193-a65f151d6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ov_post_quant_conv = mo.convert_model(str(AUTOENCODER_PATH / 'post_quant_conv.onnx'))\n",
    "# serialize(ov_post_quant_conv, str(AUTOENCODER_PATH / 'post_quant_conv.xml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f3d7e-b458-4c75-8fac-a1ab2e50224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "            'ViT-H-14', device=torch.device('cpu'), pretrained=os.path.join(p.model.model_dir, p.model.config.model.model_args.ckpt_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78a2f3b-45de-437e-9df8-966b0ffa7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "def forward(self, x: torch.Tensor):\n",
    "    print(x.shape)\n",
    "    raise\n",
    "    orig_type = x.dtype\n",
    "    x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "    return x.to(orig_type)\n",
    "p.model.clip_encoder.model.ln_final.forward = partial(forward, p.model.clip_encoder.model.ln_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d867c1-566e-4901-850d-5a500026b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(text):\n",
    "    output_video_path = p({'text': text}, output_video='./output.mp4', device='cpu')[OutputKeys.OUTPUT_VIDEO]\n",
    "    return output_video_path\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate,\n",
    "    gr.Textbox(),\n",
    "    gr.Video(),\n",
    "    examples=['A panda eating bamboo on a rock.']\n",
    ")\n",
    "demo.queue().launch(share=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd8481-57c0-44fc-acde-4abdbce0f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "\n",
    "video = torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb16254",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m video\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video' is not defined"
     ]
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ca530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openvino.runtime.ie_api.CompiledModel"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p.model.sd_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
