{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc07a8a-6d77-43f7-afd3-c1852a5f21b8",
   "metadata": {},
   "source": [
    "# Mobile language assistant with MobileVLM and OpenVINO\n",
    "[MobileVLM](https://arxiv.org/abs/2312.16886) is a competent multimodal vision language model (MMVLM) targeted to run on mobile devices. It is an amalgamation of a myriad of architectural designs and techniques that are mobile-oriented, which comprises a set of language models at the scale of 1.4B and 2.7B parameters, trained from scratch, a multimodal vision model that is pre-trained in the CLIP fashion, cross-modality interaction via an efficient projector.\n",
    "\n",
    "![](https://github.com/Meituan-AutoML/MobileVLM/raw/main/assets/mobilevlm_arch.png)\n",
    "\n",
    "The MobileVLM architecture (right) utilizes MobileLLaMA (introduced in the same paper) as its language model, intakes $\\mathbf{X}_v$ and $\\mathbf{X}_q$ which are image and language instructions as respective inputs and gives $\\mathbf{Y}_a$ as the output language response. LDP refers to a lightweight downsample projector (left).\n",
    "\n",
    "See more information on official [GitHub](https://github.com/Meituan-AutoML/MobileVLM) project page and [paper](https://arxiv.org/abs/2312.16886).\n",
    "#### Table of contents:\n",
    "- [Install requirements](#Install-requirements)\n",
    "- [Clone MobileVLM repository](#Clone-MobileVLM-repository)\n",
    "- [Import required packages](#Import-required-packages)\n",
    "- [Load the model](#Load-the-model)\n",
    "- [Convert model to OpenVINO Intermediate Representation (IR)](#Convert-model-to-OpenVINO-Intermediate-Representation-(IR))\n",
    "- [Inference](#Inference)\n",
    "    - [Load OpenVINO model](#Load-OpenVINO-model)\n",
    "    - [Prepare input data](#Prepare-input-data)\n",
    "    - [Run generation process](#Run-generation-process)\n",
    "- [Interactive inference](#Interactive-inference)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05c5ef-d883-4304-a9f9-667b745d0916",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c41f7a1-e506-4935-abfd-0a6ee2f01167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"torch>=2.0.1\" \"timm>=0.9.12\" --extra-index-url \"https://download.pytorch.org/whl/cpu\"\n",
    "%pip install -q \"transformers>=4.33.1,<4.35.0\" accelerate \"sentencepiece>=0.1.99\" \"openvino>=2023.2.0\" \"nncf>=2.7.0\" ipywidgets numpy gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8ea7b-cec8-42bf-902a-26f711e6696c",
   "metadata": {},
   "source": [
    "## Clone MobileVLM repository\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1056d6c7-97ae-44b3-8d2b-3d918798345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "MOBILEVLM_REPO_DIR = Path(\"./MobileVLM\")\n",
    "if not MOBILEVLM_REPO_DIR.exists():\n",
    "    !git clone -q \"https://github.com/Meituan-AutoML/MobileVLM.git\"\n",
    "sys.path.insert(0, str(MOBILEVLM_REPO_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219dc629-3e9a-4e00-9cb2-b97f7ccc3fdd",
   "metadata": {},
   "source": [
    "## Import required packages\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4222982-c853-43fc-a522-662c47ea785d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, openvino\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import gc\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "from mobilevlm.model.mobilevlm import load_pretrained_model\n",
    "from mobilevlm.conversation import conv_templates, SeparatorStyle\n",
    "from mobilevlm.utils import (\n",
    "    disable_torch_init,\n",
    "    process_images,\n",
    "    tokenizer_image_token,\n",
    "    KeywordsStoppingCriteria,\n",
    ")\n",
    "from mobilevlm.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "import PIL\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import openvino as ov\n",
    "import nncf\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba6fb0d0-be7c-4a48-a55a-4dcf148869da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = \"./models\"\n",
    "MODEL_PATH = 'mtgv/MobileVLM-1.7B'\n",
    "\n",
    "TEMPERATURE = 0.2\n",
    "TOP_P = None\n",
    "NUM_BEAMS = 1\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "IMAGE_PATH = MOBILEVLM_REPO_DIR / \"assets\" / \"samples\" / \"demo.jpg\"\n",
    "PROMPT_STR = \"Who is the author of this book?\\nAnswer the question using a single word or phrase.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fc34a-fd93-4af0-9cce-fd7750136521",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To load the model, we use pre-defined `load_pretrained_model` function in `mobilevlm` module. It returns the model itself, tokenizer, and image processor to convert images to appropriate tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02af483-06fc-40d1-a908-562e42f0e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MODEL_PATH.split('/')[-1]\n",
    "disable_torch_init()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    tokenizer, model, image_processor, _ = load_pretrained_model(MODEL_PATH, device=\"cpu\")\n",
    "model = model.to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47411b-d601-4d96-b2a4-0e5682db7898",
   "metadata": {},
   "source": [
    "## Convert model to OpenVINO Intermediate Representation (IR)\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a290547-575e-4ee0-9a59-274548e2c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b9979-21c6-4120-a86b-a8cc7024a536",
   "metadata": {},
   "source": [
    "For reducing memory consumption, weights compression optimization can be applied using [NNCF](https://github.com/openvinotoolkit/nncf). Weight compression aims to reduce the memory footprint of a model. It can also lead to significant performance improvement for large memory-bound models, such as Large Language Models (LLMs). LLMs and other models, which require extensive memory to store the weights during inference, can benefit from weight compression in the following ways:\n",
    "\n",
    "* enabling the inference of exceptionally large models that cannot be accommodated in the memory of the device;\n",
    "\n",
    "* improving the inference performance of the models by reducing the latency of the memory access when computing the operations with weights, for example, Linear layers.\n",
    "\n",
    "[Neural Network Compression Framework (NNCF)](https://github.com/openvinotoolkit/nncf) provides 4-bit / 8-bit mixed weight quantization as a compression method primarily designed to optimize LLMs. The main difference between weights compression and full model quantization (post-training quantization) is that activations remain floating-point in the case of weights compression which leads to a better accuracy. Weight compression for LLMs provides a solid inference performance improvement which is on par with the performance of the full model quantization. In addition, weight compression is data-free and does not require a calibration dataset, making it easy to use.\n",
    "\n",
    "`nncf.compress_weights` function can be used for performing weights compression. The function accepts an OpenVINO model and other compression parameters. Compared to INT8 compression, INT4 compression improves performance even more, but introduces a minor drop in prediction quality.\n",
    "\n",
    "More details about weights compression, can be found in [OpenVINO documentation](https://docs.openvino.ai/2023.1/weight_compression.html).\n",
    "\n",
    "Please select below whether you would like to run INT4 weight compression instead of INT8 weight compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ac142b-e8bb-49ca-aa05-d0a0899daf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca56ff04506a463fae4e6c1f221f8d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Compression mode:', options=('INT4', 'INT8'), value='INT4')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_mode = widgets.Dropdown(\n",
    "    options=['INT4', 'INT8'],\n",
    "    value='INT4',\n",
    "    description='Compression mode:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "compression_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ff7d623-69e6-4fcb-8f73-7c29f5dae9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if compression_mode.value == 'INT4':\n",
    "    wc_parameters = dict(mode=nncf.CompressWeightsMode.INT4_ASYM, group_size=128, ratio=0.8)\n",
    "else:\n",
    "    wc_parameters = dict(mode=nncf.CompressWeightsMode.INT8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42c6d11e-8c21-423f-9d25-6703fb288560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None\n",
    "    ):\n",
    "        outputs = self.model.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        logits = self.model.lm_head(hidden_states)\n",
    "\n",
    "        return (logits,) + outputs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cfa1256-66af-40ac-b0d3-9fd7aeac54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input_names(model, past_key_values):\n",
    "    input_names = [\n",
    "        \"input_ids\",\n",
    "        \"attention_mask\",\n",
    "        *itertools.chain.from_iterable(\n",
    "            [f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"]\n",
    "            for idx, _ in enumerate(past_key_values)\n",
    "        ),\n",
    "    ]\n",
    "    assert len(input_names) == len(model.inputs)\n",
    "    for _input, input_name in zip(model.inputs, input_names):\n",
    "        _input.get_tensor().set_names({input_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfbaa52b-7bf5-4239-a3ca-1ed78b04711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_output_names(model, past_key_values):\n",
    "    output_names = [\n",
    "        \"logits\",\n",
    "        *itertools.chain.from_iterable(\n",
    "            [f\"present.{idx}.key\", f\"present.{idx}.value\"]\n",
    "            for idx, _ in enumerate(past_key_values)\n",
    "        ),\n",
    "    ]\n",
    "    assert len(output_names) == len(model.outputs)\n",
    "    for out, out_name in zip(ov_model.outputs, output_names):\n",
    "        out.get_tensor().set_names({out_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "653409bb-3958-4e5f-a4a7-625a28320fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = {\n",
    "    \"inputs_embeds\": torch.zeros((1, 205, 2048)),\n",
    "    \"attention_mask\": torch.ones((1, 205), dtype=torch.long),\n",
    "}\n",
    "\n",
    "wrapped = ModelWrapper(model)\n",
    "past_key_values = wrapped(**example_input)[1]\n",
    "\n",
    "if not Path(f\"{MODELS_DIR}/stage1.xml\").exists():\n",
    "    ov_model = ov.convert_model(wrapped, example_input=example_input)\n",
    "    set_output_names(ov_model, past_key_values)\n",
    "    ov_model = nncf.compress_weights(ov_model, **wc_parameters)\n",
    "    ov.save_model(ov_model, f\"{MODELS_DIR}/stage1.xml\")\n",
    "    cleanup_torchscript_cache()\n",
    "    del ov_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ede1f11-44f4-4425-b991-d408e16e954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = {\n",
    "    \"input_ids\": torch.ones((1, 1), dtype=torch.long),\n",
    "    \"past_key_values\": past_key_values,\n",
    "    \"attention_mask\": torch.ones((1, past_key_values[-1][-1].shape[-2] + 1), dtype=torch.long),\n",
    "}\n",
    "\n",
    "if not Path(f\"{MODELS_DIR}/stage2.xml\").exists():\n",
    "    ov_model = ov.convert_model(\n",
    "        wrapped,\n",
    "        example_input=example_input,\n",
    "    )\n",
    "    set_input_names(ov_model, past_key_values)\n",
    "    set_output_names(ov_model, past_key_values)\n",
    "    ov_model = nncf.compress_weights(ov_model, **wc_parameters)\n",
    "    ov.save_model(ov_model, f\"{MODELS_DIR}/stage2.xml\")\n",
    "    cleanup_torchscript_cache()\n",
    "    del ov_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d235f796-f697-48a2-abe5-41fd10ce340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_inputs_labels_for_multimodal = model.prepare_inputs_labels_for_multimodal\n",
    "prepare_inputs_for_generation = model.prepare_inputs_for_generation\n",
    "config = model.config\n",
    "config.save_pretrained(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0c9e203-79d2-4cd3-afcf-2eb9b83cb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wrapped\n",
    "del model\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc864b13-c531-4324-abac-ecfa5fa19272",
   "metadata": {},
   "source": [
    "## Inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "`OVMobileLlamaForCausalLM` class provides ease-to-use interface for using model in generation scenario. It is based on `transformers.generation.GenerationMixin` that gives us opportunity to reuse all reach capabilities for generation implemented in HuggingFace Transformers library. More details about this interface can be found in [HuggingFace documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60fe69b9-520d-45b4-8a35-e191a87847de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OVMobileLlamaForCausalLM(transformers.GenerationMixin):\n",
    "    def __init__(self, stage1_path, stage2_path, device):\n",
    "        stage1_path = Path(stage1_path)\n",
    "        stage2_path = Path(stage2_path)\n",
    "        self.stage1 = core.compile_model(stage1_path, device)\n",
    "        self.stage2 = core.read_model(stage2_path)\n",
    "\n",
    "        self.generation_config = transformers.GenerationConfig.from_model_config(config)\n",
    "        self.config = transformers.AutoConfig.from_pretrained(MODELS_DIR)\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.prepare_inputs_for_generation = prepare_inputs_for_generation\n",
    "        self.num_pkv = 2\n",
    "        self.input_names = {key.get_any_name(): idx for idx, key in enumerate(self.stage2.inputs)}\n",
    "        self.output_names = {key.get_any_name(): idx for idx, key in enumerate(self.stage2.outputs)}\n",
    "        self.key_value_input_names = [key for key in self.input_names if \"key_values\" in key]\n",
    "        self.key_value_output_names = [key for key in self.output_names if \"present\" in key]\n",
    "        stage2 = core.compile_model(self.stage2, device)\n",
    "        self.request = stage2.create_infer_request()\n",
    "\n",
    "    def can_generate(self):\n",
    "        \"\"\"Returns True to validate the check that the model using `GenerationMixin.generate()` can indeed generate.\"\"\"\n",
    "        return True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        images: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        prefix_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> transformers.modeling_outputs.CausalLMOutputWithPast:\n",
    "        return self.forward(input_ids, images, attention_mask, prefix_mask, past_key_values)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        images: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        prefix_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> transformers.modeling_outputs.CausalLMOutputWithPast:\n",
    "        \"\"\"General inference method\"\"\"\n",
    "        inputs = {}\n",
    "        if past_key_values is not None:\n",
    "            # Flatten the past_key_values\n",
    "            attention_mask = torch.ones(\n",
    "                (input_ids.shape[0], past_key_values[-1][-1].shape[-2] + 1),\n",
    "                dtype=input_ids.dtype,\n",
    "            )\n",
    "            past_key_values = tuple(\n",
    "                past_key_value\n",
    "                for pkv_per_layer in past_key_values\n",
    "                for past_key_value in pkv_per_layer\n",
    "            )\n",
    "            # Add the past_key_values to the decoder inputs\n",
    "            inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "\n",
    "        else:\n",
    "            return self.forward_with_image(input_ids, images, attention_mask)\n",
    "        inputs[\"input_ids\"] = np.array(input_ids)\n",
    "\n",
    "        if \"attention_mask\" in self.input_names:\n",
    "            inputs[\"attention_mask\"] = np.array(attention_mask)\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs, share_inputs=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(\"logits\").data)\n",
    "\n",
    "        # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "        past_key_values = tuple(\n",
    "            self.request.get_tensor(key).data for key in self.key_value_output_names\n",
    "        )\n",
    "        # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "\n",
    "        past_key_values = tuple(\n",
    "            past_key_values[i : i + self.num_pkv]\n",
    "            for i in range(0, len(past_key_values), self.num_pkv)\n",
    "        )\n",
    "\n",
    "        return transformers.modeling_outputs.CausalLMOutputWithPast(\n",
    "            logits=logits, past_key_values=past_key_values\n",
    "        )\n",
    "\n",
    "    def forward_with_image(self, input_ids, images, attention_mask):\n",
    "        \"\"\"First step inference method, that resolves multimodal data\"\"\"\n",
    "        _, attention_mask, _, input_embed, _ = prepare_inputs_labels_for_multimodal(\n",
    "            input_ids, attention_mask, images=images, past_key_values=None, labels=None\n",
    "        )\n",
    "        outs = self.stage1({\"inputs_embeds\": input_embed, \"attention_mask\": attention_mask})\n",
    "        logits = outs[0]\n",
    "        pkv = list(outs.values())[1:]\n",
    "        pkv = tuple(pkv[i : i + self.num_pkv] for i in range(0, len(pkv), self.num_pkv))\n",
    "        return transformers.modeling_outputs.CausalLMOutputWithPast(\n",
    "            logits=torch.from_numpy(logits), past_key_values=pkv\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546d183-1e22-4801-a57d-f58b70a69061",
   "metadata": {},
   "source": [
    "Now, when we have model and defined generation pipeline, we can run model inference.\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2044b7-449b-4446-bde1-39ba9cf95475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b95c8fdbb4440dcabe5b473c05365fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1996220-bc65-4e10-bd08-c50db96de13c",
   "metadata": {},
   "source": [
    "### Load OpenVINO model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee3ff877-1769-4692-9421-d958a0efbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = OVMobileLlamaForCausalLM(\n",
    "    f\"{MODELS_DIR}/stage1.xml\", f\"{MODELS_DIR}/stage2.xml\", device.value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36be6844-e00d-4611-85da-31ad76163062",
   "metadata": {},
   "source": [
    "### Prepare input data\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a496153-4580-4343-98c0-f85c13be0fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [PIL.Image.open(IMAGE_PATH).convert(\"RGB\")]\n",
    "images_tensor = process_images(\n",
    "    images, image_processor, transformers.AutoConfig.from_pretrained(MODELS_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff8dc273-8ffa-4ffb-b14a-42e1d2144466",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv_templates[\"v1\"].copy()\n",
    "conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + \"\\n\" + PROMPT_STR)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "input_ids = tokenizer_image_token(\n",
    "    prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
    ").unsqueeze(0)\n",
    "stopping_criteria = KeywordsStoppingCriteria([stop_str], tokenizer, input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f2c18-fc6c-48ac-8dae-9e5356a9ffad",
   "metadata": {},
   "source": [
    "### Run generation process\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79789ce6-ddff-4544-a5d0-7d3614450037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MobileVLM-1.7B with OpenVINO: Susan Wise Bauer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_ids = ov_model.generate(\n",
    "    input_ids,\n",
    "    images=images_tensor,\n",
    "    do_sample=True if TEMPERATURE > 0 else False,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    use_cache=True,\n",
    "    stopping_criteria=[stopping_criteria],\n",
    ")\n",
    "input_token_len = input_ids.shape[1]\n",
    "n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n",
    "if n_diff_input_output > 0:\n",
    "    print(f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\")\n",
    "outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n",
    "outputs = outputs.strip()\n",
    "if outputs.endswith(stop_str):\n",
    "    outputs = outputs[: -len(stop_str)]\n",
    "print(f\"🚀 {model_name} with OpenVINO: {outputs.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb76b8-24f7-4d55-ad32-0de282c2137d",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe048d2-0bd4-4748-aaf5-af221e069333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(img, prompt):\n",
    "    images_tensor = process_images(\n",
    "        [img], image_processor, transformers.AutoConfig.from_pretrained(MODELS_DIR)\n",
    "    )\n",
    "    prompt = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "    conv = conv_templates[\"v1\"].copy()\n",
    "    conv.append_message(conv.roles[0], prompt)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    prompt = conv.get_prompt()\n",
    "    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n",
    "    input_ids = tokenizer_image_token(\n",
    "        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
    "    ).unsqueeze(0)\n",
    "    stopping_criteria = KeywordsStoppingCriteria([stop_str], tokenizer, input_ids)\n",
    "\n",
    "    output_ids = ov_model.generate(\n",
    "        input_ids,\n",
    "        images=images_tensor,\n",
    "        do_sample=True if TEMPERATURE > 0 else False,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "    input_token_len = input_ids.shape[1]\n",
    "    outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n",
    "    outputs = outputs.strip()\n",
    "    if outputs.endswith(stop_str):\n",
    "        outputs = outputs[: -len(stop_str)]\n",
    "\n",
    "    return outputs.strip()\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate,\n",
    "    [gr.Image(label=\"Image\", type=\"pil\"), gr.Textbox(label=\"Prompt\")],\n",
    "    gr.Textbox(),\n",
    "    examples=[\n",
    "        [\n",
    "            str(IMAGE_PATH),\n",
    "            PROMPT_STR,\n",
    "        ]\n",
    "    ],\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
