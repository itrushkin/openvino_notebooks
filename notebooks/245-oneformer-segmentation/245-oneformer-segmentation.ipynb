{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82044cea-a95d-4812-89a2-bbb055ea1661",
   "metadata": {},
   "source": [
    "# Universal Segmentation with OneFormer and OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbb8aa-c070-4e7d-8b7a-64588e7f0da4",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to use the [OneFormer](https://arxiv.org/abs/2211.06220) model from HuggingFace with OpenVINO. It describes how to download weights and create PyTorch model using Hugging Face transformers library, then convert model to OpenVINO Intermediate Representation format (IR) using OpenVINO Model Optimizer API and run model inference\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_architecture.png)\n",
    "\n",
    "OneFormer is a follow-up work of [Mask2Former](https://arxiv.org/abs/2112.01527). The latter still requires training on instance/semantic/panoptic datasets separately to get state-of-the-art results.\n",
    "\n",
    "OneFormer incorporates a text module in the Mask2Former framework, to condition the model on the respective subtask (instance, semantic or panoptic). This gives even more accurate results, but comes with a cost of increased latency, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9e61e-e3d9-486b-a652-13083530fbc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e1b9f3-faf1-4260-aadf-c9edd53e53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openvino-dev==2023.1.0.dev20230728 gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc745f-0960-4b17-8559-dd8daeac8318",
   "metadata": {},
   "source": [
    "## Prepare the environment\n",
    "Import all required packages and set paths for models and constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1099fec8-3e7b-4699-b949-a166547a1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from transformers.models.oneformer.modeling_oneformer import OneFormerForUniversalSegmentationOutput\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a38c4a-433f-4fed-bf5a-410be7160f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_PATH = Path(\"oneformer.onnx\")\n",
    "IR_PATH = Path(\"oneformer.xml\")\n",
    "OUTPUT_NAMES = ['class_queries_logits', 'masks_queries_logits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa6aa4-5730-469b-8fbe-2bece6ef1641",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load OneFormer fine-tuned on COCO for universal segmentation\n",
    "Here we use the `from_pretrained` method of `OneFormerForUniversalSegmentation` to load the Swin-L model trained on COCO dataset.\n",
    "\n",
    "Also, we use HuggingFace processor to prepare the model inputs from images and post-process model outputs for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca18e5a3-34dd-466b-b08f-5cf0d6069e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_coco_swin_large\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\n",
    "    \"shi-labs/oneformer_coco_swin_large\",\n",
    "    torchscript=True\n",
    ")\n",
    "id2label = model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08092ec-addc-4cea-81ab-b72153cd6190",
   "metadata": {},
   "source": [
    "## Convert PyTorch model to ONNX\n",
    "\n",
    "In order to convert PyTorch model to OpenVINO IR, we should convert it into ONNX representation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fb1caf-397a-4149-9dfe-641a47b3a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_seq_length = processor.task_seq_length\n",
    "shape = (800, 800)\n",
    "dummy_input = {\n",
    "    \"pixel_values\": torch.randn(1, 3, *shape),  # TODO: make shapes dynamic\n",
    "    \"task_inputs\": torch.randn(1, task_seq_length),\n",
    "    \"pixel_mask\": torch.randn(1, *shape),  # TODO: make shapes dynamic\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4b09fd-dfd8-45bd-be93-992778ca8343",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Convert the model to OpenVINO IR format\n",
    "While ONNX models are directly supported by OpenVINO runtime, it can be useful to convert them to IR format to take the advantage of OpenVINO optimization tools and features. The `mo.convert_model` python function in OpenVINO Model Optimizer can be used for converting the model. The function returns instance of OpenVINO Model class, which is ready to use in Python interface. However, it can also be serialized to OpenVINO IR format for future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16648a3-fa3c-46d5-b2dd-53646c5f9b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Please fix your imports. Module %s has been moved to %s. The old module will be deleted in version %s.\n"
     ]
    }
   ],
   "source": [
    "if not IR_PATH.exists():\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = mo.convert_model(model, example_input=dummy_input)\n",
    "    serialize(model, IR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2472a04-a13b-465c-9f42-9f961d7d7907",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can prepare the image using the HuggingFace processor. OneFormer leverages a processor which internally consists of an image processor (for the image modality) and a tokenizer (for the text modality). OneFormer is actually a multimodal model, since it incorporates both images and text to solve image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3beb94c-03b5-4f91-b44b-350ace719c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_inputs(image: Image.Image, task: str):\n",
    "    \"\"\"Convert image to model input\"\"\"\n",
    "    image = ImageOps.pad(image, shape)\n",
    "    inputs = processor(image, [task], return_tensors=\"pt\")\n",
    "    converted = {\n",
    "        'pixel_values': inputs['pixel_values'],\n",
    "        'task_inputs': inputs['task_inputs']\n",
    "    }\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c52ac9-021d-4cbe-8a73-61c1f555220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(d):\n",
    "    \"\"\"Convert OpenVINO model output to HuggingFace representation for visualization\"\"\"\n",
    "    hf_kwargs = {\n",
    "        output_name: torch.tensor(d[output_name]) for output_name in OUTPUT_NAMES\n",
    "    }\n",
    "\n",
    "    return OneFormerForUniversalSegmentationOutput(**hf_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7248344-3579-4016-a07c-027251c749c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()\n",
    "# Read the model from files.\n",
    "model = core.read_model(model=IR_PATH)\n",
    "# Compile the model for a specific device.\n",
    "model = core.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661c9b8-e08c-48a9-ab22-4d6fddab572a",
   "metadata": {},
   "source": [
    "Model predicts `class_queries_logits` of shape `(batch_size, num_queries)`\n",
    "and `masks_queries_logits` of shape `(batch_size, num_queries, height, width)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3ab54-4f57-40ee-b32e-b2fff72b77af",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "Here we define functions for visualization of network outputs to show the inference results in Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dc3ec39-83ed-4fd8-8ff8-8975b027d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    @staticmethod\n",
    "    def extract_legend(handles):\n",
    "        fig = plt.figure()\n",
    "        fig.legend(handles=handles, ncol=len(handles) // 20 + 1, loc='center')\n",
    "        fig.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    @staticmethod\n",
    "    def predicted_semantic_map_to_figure(predicted_map):\n",
    "        segmentation = predicted_map[0]\n",
    "        # get the used color map\n",
    "        viridis = plt.get_cmap('viridis', torch.max(segmentation))\n",
    "        # get all the unique numbers\n",
    "        labels_ids = torch.unique(segmentation).tolist()\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(segmentation)\n",
    "        ax.set_axis_off()\n",
    "        handles = []\n",
    "        for label_id in labels_ids:\n",
    "            label = id2label[label_id]\n",
    "            color = viridis(label_id)\n",
    "            handles.append(mpatches.Patch(color=color, label=label))\n",
    "        fig_legend = Visualizer.extract_legend(handles=handles)\n",
    "        fig.tight_layout()\n",
    "        return fig, fig_legend\n",
    "        \n",
    "    @staticmethod\n",
    "    def predicted_instance_map_to_figure(predicted_map):\n",
    "        segmentation = predicted_map[0]['segmentation']\n",
    "        segments_info = predicted_map[0]['segments_info']\n",
    "        # get the used color map\n",
    "        viridis = plt.get_cmap('viridis', torch.max(segmentation))\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(segmentation)\n",
    "        ax.set_axis_off()\n",
    "        instances_counter = defaultdict(int)\n",
    "        handles = []\n",
    "        # for each segment, draw its legend\n",
    "        for segment in segments_info:\n",
    "            segment_id = segment['id']\n",
    "            segment_label_id = segment['label_id']\n",
    "            segment_label = id2label[segment_label_id]\n",
    "            label = f\"{segment_label}-{instances_counter[segment_label_id]}\"\n",
    "            instances_counter[segment_label_id] += 1\n",
    "            color = viridis(segment_id)\n",
    "            handles.append(mpatches.Patch(color=color, label=label))\n",
    "            \n",
    "        fig_legend = Visualizer.extract_legend(handles)\n",
    "        fig.tight_layout()\n",
    "        return fig, fig_legend\n",
    "\n",
    "    @staticmethod\n",
    "    def predicted_panoptic_map_to_figure(predicted_map):\n",
    "        segmentation = predicted_map[0]['segmentation']\n",
    "        segments_info = predicted_map[0]['segments_info']\n",
    "        # get the used color map\n",
    "        viridis = plt.get_cmap('viridis', torch.max(segmentation))\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(segmentation)\n",
    "        ax.set_axis_off()\n",
    "        instances_counter = defaultdict(int)\n",
    "        handles = []\n",
    "        # for each segment, draw its legend\n",
    "        for segment in segments_info:\n",
    "            segment_id = segment['id']\n",
    "            segment_label_id = segment['label_id']\n",
    "            segment_label = id2label[segment_label_id]\n",
    "            label = f\"{segment_label}-{instances_counter[segment_label_id]}\"\n",
    "            instances_counter[segment_label_id] += 1\n",
    "            color = viridis(segment_id)\n",
    "            handles.append(mpatches.Patch(color=color, label=label))\n",
    "            \n",
    "        fig_legend = Visualizer.extract_legend(handles)\n",
    "        fig.tight_layout()\n",
    "        return fig, fig_legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d70a38-e6ee-464f-8d72-3f4f9d05aee6",
   "metadata": {
    "test_replace": {
      "    demo.launch(debug=True)": "    demo.launch()",
      "    demo.launch(share=True, debug=True)": "    demo.launch(share=True)"  
    }
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def segment(img: Image.Image, task: str):\n",
    "    \"\"\"\n",
    "    Apply segmentation on an image.\n",
    "\n",
    "    Args:\n",
    "        img: Input image. It will be resized to 800x800.\n",
    "        task: String describing the segmentation task. Supported values are: \"semantic\", \"instance\" and \"panoptic\".\n",
    "    Returns:\n",
    "        Tuple[Figure, Figure]: Segmentation map and legend charts.\n",
    "    \"\"\"\n",
    "    if img is None:\n",
    "        raise gr.Error('Please load the image or use one from the examples list')\n",
    "    inputs = prepare_inputs(img, task)\n",
    "    outputs = model(inputs)\n",
    "    hf_output = process_output(outputs)\n",
    "    predicted_map = getattr(processor, f'post_process_{task}_segmentation')(hf_output, target_sizes=[img.size[::-1]])\n",
    "    return getattr(Visualizer, f'predicted_{task}_map_to_figure')(predicted_map)\n",
    "    \n",
    "demo = gr.Interface(\n",
    "    segment,\n",
    "    [\n",
    "        gr.Image(label=\"Image\", type=\"pil\"),\n",
    "        gr.Radio([\"semantic\", \"instance\", \"panoptic\"], label=\"Task\", value=\"semantic\"),\n",
    "    ],\n",
    "    [gr.Plot(label=\"Result\"), gr.Plot(label=\"Legend\")],\n",
    "    examples=[[\"sample.jpg\", \"semantic\"]],\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(share=True, debug=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
