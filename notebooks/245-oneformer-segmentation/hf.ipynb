{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099fec8-3e7b-4699-b949-a166547a1081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from openvino.tools import mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e62f4-095f-42b4-8845-290f8ebfd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    \"\"\" Model wrapper that unpacks forward arguments from single enumerable of tensors.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.keywords = [\n",
    "            'pixel_values',\n",
    "            'task_inputs',\n",
    "            'text_inputs',\n",
    "            'mask_labels',\n",
    "            'class_labels',\n",
    "            'pixel_mask',\n",
    "            'output_auxiliary_logits',\n",
    "            'output_hidden_states',\n",
    "            'output_attentions',\n",
    "            'return_dict'\n",
    "        ]\n",
    "    \n",
    "    def forward(self, *tensors):\n",
    "        kwargs = {}\n",
    "        for kw, tensor in zip(self.keywords, tensors):\n",
    "            kwargs[kw] = None if tensor.isnan().all() else tensor\n",
    "        return self.model.forward(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9842e-f9b2-452a-9c0f-a38e05f93903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OneFormer fine-tuned on ADE20k for universal segmentation\n",
    "processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n",
    "model = MyModel(model)\n",
    "model = mo.convert_model(model, example_input=(\n",
    "    torch.randn(1, 3, 512, 683), # pixel_values\n",
    "    torch.randn(1, 77), # task_inputs\n",
    "    torch.tensor([float('nan')]), # text_inputs\n",
    "    torch.tensor([float('nan')]), # mask_labels\n",
    "    torch.tensor([float('nan')]), # class_labels\n",
    "    torch.randn(1, 512, 683), # pixel_mask\n",
    "    torch.tensor([float('nan')]), # output_auxiliary_logits\n",
    "    torch.tensor([float('nan')]), # output_hidden_states\n",
    "    torch.tensor([float('nan')]), # output_attentions\n",
    "    torch.tensor([float('nan')])), # return_dict\n",
    "    onnx_opset_version=11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2f518-769a-4c2f-9781-4da5bb8b5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    \"https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg\"\n",
    ")\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be7e32-5ac0-4d2a-bfa7-3cac5b07934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Semantic Segmentation\n",
    "inputs = processor(image, [\"semantic\"], return_tensors=\"pt\")\n",
    "for k,v in inputs.items():\n",
    "    print(f'{k}: {v.shape}')\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for semantic postprocessing\n",
    "predicted_semantic_map = processor.post_process_semantic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d1c921-79b9-4e8d-a6e2-627b1b8750ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def draw_semantic_segmentation(segmentation):\n",
    "    # get the used color map\n",
    "    viridis = cm.get_cmap('viridis', torch.max(segmentation))\n",
    "    # get all the unique numbers\n",
    "    labels_ids = torch.unique(segmentation).tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(segmentation)\n",
    "    handles = []\n",
    "    for label_id in labels_ids:\n",
    "        label = model.config.id2label[label_id]\n",
    "        color = viridis(label_id)\n",
    "        handles.append(mpatches.Patch(color=color, label=label))\n",
    "    ax.legend(handles=handles)\n",
    "\n",
    "draw_semantic_segmentation(predicted_semantic_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcf98d-656d-427b-9517-6b10ad420aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Segmentation\n",
    "inputs = processor(image, [\"instance\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for instance postprocessing\n",
    "predicted_instance_map = processor.post_process_instance_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "f\"ðŸ‘‰ Instance Predictions Shape: {list(predicted_instance_map.shape)}\"\n",
    "'ðŸ‘‰ Instance Predictions Shape: [512, 683]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43283b0d-fa89-453f-aa4c-026ba3ec4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panoptic Segmentation\n",
    "inputs = processor(image, [\"panoptic\"], return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "# model predicts class_queries_logits of shape `(batch_size, num_queries)`\n",
    "# and masks_queries_logits of shape `(batch_size, num_queries, height, width)`\n",
    "class_queries_logits = outputs.class_queries_logits\n",
    "masks_queries_logits = outputs.masks_queries_logits\n",
    "\n",
    "# you can pass them to processor for panoptic postprocessing\n",
    "predicted_panoptic_map = processor.post_process_panoptic_segmentation(\n",
    "    outputs, target_sizes=[image.size[::-1]]\n",
    ")[0][\"segmentation\"]\n",
    "f\"ðŸ‘‰ Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}\"\n",
    "'ðŸ‘‰ Panoptic Predictions Shape: [512, 683]'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
